<!doctype html><html lang=zh><head><title>记一次kubelet 内存泄漏 | easystack</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="kubelet memroy leak"><meta name=generator content="Hugo 0.97.3"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon></head><body><nav class=navigation><a href=/><span class=arrow>←</span>首页</a>
<a href=/posts>归档</a>
<a href=/tags>标签</a>
<a href=/about>关于</a>
<a class=button href=https://easystack.github.io/index.xml>订阅</a></nav><main class=main><section id=single><h1 class=title>记一次kubelet 内存泄漏</h1><div class=tip><time datetime="2021-11-09 15:50:43 +0800 +0800">2021年11月09日</time>
<span class=split>·</span>
<span>2020字</span>
<span class=split>·</span>
<span>5分钟</span></div><aside class=toc><details><summary>Table of Contents</summary><div><nav id=TableOfContents><ul><li><a href=#基础>基础</a></li></ul><ul><li><a href=#基础-1>基础</a></li></ul><ul><li><a href=#exec-流程>exec 流程</a></li></ul><ul><li><a href=#ttrpc>ttrpc</a></li></ul></nav></div></details></aside><div class=content><h1 id=背景>背景 <a href=#%e8%83%8c%e6%99%af class=anchor>🔗</a></h1><p>一次生产环境中，发现 kubelet 内存达到上 GB 以上，这个不符合平常使用情况，首先想到的是内存泄漏，那么肯定使用 pprof 以及调查为什么会产生内存泄漏</p><h1 id=profile>profile <a href=#profile class=anchor>🔗</a></h1><h2 id=基础>基础 <a href=#%e5%9f%ba%e7%a1%80 class=anchor>🔗</a></h2><ul><li>cpu <code>/debug/pprof/profile</code>，主要分析耗时和优化算法，得到 profile 文件</li><li>heap: <code>/debug/pprof/heap</code>，查看活动对象的内存分配情况，得到 profile 文件</li><li>threadcreate: <code>/debug/pprof/threadcreate</code>, 线程创建概况报告程序中导致创建新的操作系统线程的部分</li><li>goroutine： 报告所有当前 goroutine 的堆栈信息，<strong>没有 profile 文件</strong></li><li>trace: <code>/debug/pprof/trace</code> 当前程序的执行跟踪，go tool trace 中使用</li></ul><h1 id=kubelet>kubelet <a href=#kubelet class=anchor>🔗</a></h1><p>查看平台的内存使用情况，如下
注：（以下数据非当时环境，是之后重新复现后取的，比发生泄漏环境要低的多)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># top</span>
</span></span><span style=display:flex><span>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
</span></span><span style=display:flex><span><span style=color:#666>31599</span> root      <span style=color:#666>20</span>   <span style=color:#666>0</span> <span style=color:#666>2877396</span> 983.5m  <span style=color:#666>66424</span> S   4.3  6.2  28:00.93 kubelet           
</span></span></code></pre></div><p>kubelet 数据默认打开 pprof， 通过 <a href=https://github.com/google/pprof target=_blank rel=noopener>pprof</a> 拿到数据 , 通过以下命令方式</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># pprof -tls_ca {cafile} -tls_key {keyfile} -tls_cert {certfile}</span>
</span></span><span style=display:flex><span> https://<span style=color:#666>{</span>host<span style=color:#666>}</span>:10250/debug/pprof/heap
</span></span></code></pre></div><p>拿到数据后，还可以拿到正常节点的数据做对比，如下</p><p>正常 kubelet 内存使用<p class=markdown-image><img src=https://s2.loli.net/2022/04/21/SLKZQjG9Yvdx4Wk.png alt></p></p><p>不正常 kubelet 内存使用<p class=markdown-image><img src=https://s2.loli.net/2022/04/21/jCefqQ7JARYrSno.png alt></p></p><h2 id=基础-1>基础 <a href=#%e5%9f%ba%e7%a1%80-1 class=anchor>🔗</a></h2><p>描述下 kubelet 当前目录和会涉及的代码片段</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>pkg/kubelet/
</span></span><span style=display:flex><span>├── cri #cri接口
</span></span><span style=display:flex><span>├── server # http hanlder入口
</span></span><span style=display:flex><span>├── stats # 容器 cpu/memory，filesystem info 抓取
</span></span><span style=display:flex><span>├── kuberuntime # 桥接容器运行时 和 kubelet操作
</span></span><span style=display:flex><span>└── cm #container manager缩写，控制器内容包括 cgroup，topology,device等
</span></span></code></pre></div><p>可以直接跨越到 cri/ 内查看 当前接口实现，未设置超时时间</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>func</span> (r <span style=color:#666>*</span>remoteRuntimeService) <span style=color:#00a000>ListContainerStats</span>(filter <span style=color:#666>*</span>runtimeapi.ContainerStatsFilter) ([]<span style=color:#666>*</span>runtimeapi.ContainerStats, <span style=color:#0b0;font-weight:700>error</span>) {
</span></span><span style=display:flex><span>	<span style=color:#080;font-style:italic>// Do not set timeout, because writable layer stats collection takes time.
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>	<span style=color:#080;font-style:italic>// TODO(random-liu): Should we assume runtime should cache the result, and set timeout here?
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>	ctx, cancel <span style=color:#666>:=</span> <span style=color:#00a000>getContextWithCancel</span>()
</span></span><span style=display:flex><span>	<span style=color:#a2f;font-weight:700>defer</span> <span style=color:#00a000>cancel</span>()
</span></span><span style=display:flex><span>	<span style=color:#080;font-style:italic>// 这里未设置超时时间
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>	resp, err <span style=color:#666>:=</span> r.runtimeClient.<span style=color:#00a000>ListContainerStats</span>(ctx, <span style=color:#666>&amp;</span>runtimeapi.ListContainerStatsRequest{
</span></span><span style=display:flex><span>		Filter: filter,
</span></span><span style=display:flex><span>	})
</span></span></code></pre></div><p>当前调用链大致如下 , 可以看到及时客户端退出，但是 kubelet 到 containerd 的连接还是会保持，直到拿到数据为止<p class=markdown-image><img src=https://s2.loli.net/2022/04/26/bM1swGoBIDTx3gF.png alt></p></p><p>那么考虑就是如何复现这个问题，但通过测试代码 [1] 并未复现问题，这里其实一直是阻塞调查的地方，那么不妨换个思路，现继续挖下为什么 containerd 没有返回数据呢？</p><p>到这里，其实可以发现 kubelet 都集中在 <code>stats.ListPodStats</code> 花费上，该调用会最终到 containerd 的 Stats 接口上，那么应该继续分析 containerd ，这应该是产生问题的根本原因</p><h1 id=containerd>containerd <a href=#containerd class=anchor>🔗</a></h1><p>通过抓取 heap 和 goroutine 信息分析，而我们环境中 containerd 的 debug socket 已经打开，所以比较方便拿到 containerd profile 信息</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># ctr pprof heap &gt; heap.profile</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># ctr pprof goroutines &gt; goroutine.profile</span>
</span></span></code></pre></div><p>拿到数据后，还可以拿到正常节点的数据做对比</p><p>正常 containerd 内存使用<p class=markdown-image><img src=https://s2.loli.net/2022/04/21/e1gxopz2HduOiQM.png alt></p></p><p>不正常 containerd 内存使用<p class=markdown-image><img src=https://s2.loli.net/2022/04/21/qSt6okRhXIEYWUZ.png alt></p></p><p>看到的是 SPDY 内存消耗较多，我们知道 SPDY 是 HTTP/2 前身，主要用于流式连接，当前主要是 接口 exec, attach, portfoward 在接口上，以下是简单对 exec 的代码理解 , 然后使用了 exec 确能稳定复现问题，复现代码如下</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#080>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#080></span><span style=color:#b8860b>i</span><span style=color:#666>=</span><span style=color:#666>0</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NUM</span><span style=color:#666>=</span><span style=color:#b8860b>$1</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>while</span> <span style=color:#666>[</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$i</span><span style=color:#b44>&#34;</span> -lt <span style=color:#b8860b>$NUM</span> <span style=color:#666>]</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span>   <span style=color:#666>(</span>kubectl <span style=color:#a2f>exec</span> -i test-74cf75b654-gw5hk -- ls /opt<span style=color:#666>)</span>&amp;
</span></span><span style=display:flex><span>   <span style=color:#a2f>let</span> <span style=color:#b44>&#34;i += 1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div><h2 id=exec-流程>exec 流程 <a href=#exec-%e6%b5%81%e7%a8%8b class=anchor>🔗</a></h2><p>流程需要从 kubelet 开始分析 , 通过前文中对 kubelet 目录的简单介绍，那么入口肯定是在 server 目录内，直接到主题 Exec，对应函数是 <code>getExec(request *restful.Request, response *restful.Response)</code> ， 行为简单描述如下
- 校验参数，通常 stdout/stderr 为 true，目前常用的是 <code>-it</code>，也就是需要配置 stdin 和 ttry 是否为 true
- 查询 pod 当前是否支持 exec
- 执行 GetExec 获取 url
- 创建代理，转发 apiserver 数据 stdin 和 stdout</p><p>cri 服务 主要分为两部分，重点描述 ServerExec
- GetExec：创建 url 路由，增加 token <code>host:port/exec/{token}</code>
- ServerExec： 创建 task 和 process，并绑定标准输入和输出等</p><p>ServerExec</p><ul><li>升级 http 为 SPDY stream，看上去 spdy 是比较重，因为要至少建立三个 goroutine <code>handler.waitForStreams(streamCh, expectedStreams, expired.C)</code></li><li>执行 Exec (在 streamRuntime 内)，等待 process 结束，或者上游退出</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#080;font-style:italic>//创建Task， task实际是 containerd/containerd/task.go 类型
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>task, err <span style=color:#666>:=</span> container.<span style=color:#00a000>Task</span>(ctx, <span style=color:#a2f;font-weight:700>nil</span>)
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>if</span> err <span style=color:#666>!=</span> <span style=color:#a2f;font-weight:700>nil</span> {
</span></span><span style=display:flex><span>	<span style=color:#a2f;font-weight:700>return</span> <span style=color:#a2f;font-weight:700>nil</span>, errors.<span style=color:#00a000>Wrap</span>(err, <span style=color:#b44>&#34;failed to load task&#34;</span>)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>pspec <span style=color:#666>:=</span> spec.Process
</span></span><span style=display:flex><span>pspec.Args = opts.cmd
</span></span><span style=display:flex><span>pspec.Terminal = opts.tty
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>if</span> opts.tty {
</span></span><span style=display:flex><span>	oci.<span style=color:#00a000>WithEnv</span>([]<span style=color:#0b0;font-weight:700>string</span>{<span style=color:#b44>&#34;TERM=xterm&#34;</span>})(ctx, <span style=color:#a2f;font-weight:700>nil</span>, <span style=color:#a2f;font-weight:700>nil</span>, spec)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>volatileRootDir <span style=color:#666>:=</span> c.<span style=color:#00a000>getVolatileContainerRootDir</span>(id)
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>var</span> execIO <span style=color:#666>*</span>cio.ExecIO
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>// 创建process, 实际是 containerd/containerd/process.go 类型
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>process, err <span style=color:#666>:=</span> task.<span style=color:#00a000>Exec</span>(ctx, execID, pspec,
</span></span><span style=display:flex><span>	<span style=color:#a2f;font-weight:700>func</span>(id <span style=color:#0b0;font-weight:700>string</span>) (containerdio.IO, <span style=color:#0b0;font-weight:700>error</span>) {
</span></span><span style=display:flex><span>		<span style=color:#a2f;font-weight:700>var</span> err <span style=color:#0b0;font-weight:700>error</span>
</span></span><span style=display:flex><span>		execIO, err = cio.<span style=color:#00a000>NewExecIO</span>(id, volatileRootDir, opts.tty, opts.stdin <span style=color:#666>!=</span> <span style=color:#a2f;font-weight:700>nil</span>)
</span></span><span style=display:flex><span>		<span style=color:#a2f;font-weight:700>return</span> execIO, err
</span></span><span style=display:flex><span>	},
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>// 获取 process 退出管道
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>exitCh, err <span style=color:#666>:=</span> process.<span style=color:#00a000>Wait</span>(ctx)
</span></span><span style=display:flex><span>err <span style=color:#666>:=</span> process.<span style=color:#00a000>Start</span>(ctx)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>// 将execIO 和 http 流绑定
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic>// 内部会创建协程 stdin ，stdout, waitGroup
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span>attachDone <span style=color:#666>:=</span> execIO.<span style=color:#00a000>Attach</span>(cio.AttachOptions{
</span></span><span style=display:flex><span>	Stdin:     opts.stdin,
</span></span><span style=display:flex><span>	Stdout:    opts.stdout,
</span></span><span style=display:flex><span>	Stderr:    opts.stderr,
</span></span><span style=display:flex><span>	Tty:       opts.tty,
</span></span><span style=display:flex><span>	StdinOnce: <span style=color:#a2f;font-weight:700>true</span>,
</span></span><span style=display:flex><span>	CloseStdin: <span style=color:#a2f;font-weight:700>func</span>() <span style=color:#0b0;font-weight:700>error</span> {
</span></span><span style=display:flex><span>		<span style=color:#a2f;font-weight:700>return</span> process.<span style=color:#00a000>CloseIO</span>(ctx, containerd.WithStdinCloser)
</span></span><span style=display:flex><span>	},
</span></span><span style=display:flex><span>})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>select</span> {
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>case</span> <span style=color:#666>&lt;-</span>execCtx.<span style=color:#00a000>Done</span>():
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>case</span> exitRes <span style=color:#666>:=</span> <span style=color:#666>&lt;-</span>exitCh:
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>这里是阻塞在调用 exec 上，但是和 cpuAndMemoryStats 并无关系，如果是有关系，那么也只有在 shim 这一级，因为都要调用 shim 的 rpc 接口</p><h1 id=shim>shim <a href=#shim class=anchor>🔗</a></h1><p>当前 runc 使用的 shim 是 containerd-shim-runc-v2, 在内置的 shim server 上有一个方便调试的方式，发送 USER1 信号 可以获取 goroutines 信息，具体代码如下</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#080;font-style:italic>// runtime/v2/shim/shim_unix.go
</span></span></span><span style=display:flex><span><span style=color:#080;font-style:italic></span><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>setupDumpStacks</span>(dump <span style=color:#a2f;font-weight:700>chan</span><span style=color:#666>&lt;-</span> os.Signal) {
</span></span><span style=display:flex><span>	signal.<span style=color:#00a000>Notify</span>(dump, syscall.SIGUSR1)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>// 发送 USER1 信号
</span></span><span style=display:flex><span><span style=color:#a2f>kill</span> -10 <span style=color:#666>{</span>pid<span style=color:#666>}</span>
</span></span><span style=display:flex><span>// 获取containerd 日志，获取BEGIN goroutine stack dump 日志
</span></span><span style=display:flex><span>journalctl -eu containerd &gt;contaienrd.log
</span></span></code></pre></div><p>查看 shim 的 goroutines 信息，因为和 ttrpc 有关，那直接找 ttrpc 信息吧，可以发现以下信息，这里显然不应该有那么多 goroutine hang 在 <code>vendor/github.com/containerd/ttrpc/server.go:444</code> 行，那么继续看下 ttrpc 有关实现呢</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>$ cat shim.goroutine.profile |grep ttrpc/server | sort |uniq -c |sort
</span></span><span style=display:flex><span>      1 	vendor/github.com/containerd/ttrpc/server.go:362 +0x149
</span></span><span style=display:flex><span>      1 	vendor/github.com/containerd/ttrpc/server.go:404 +0x5ee
</span></span><span style=display:flex><span>      1 	vendor/github.com/containerd/ttrpc/server.go:431 +0x41a
</span></span><span style=display:flex><span>      1 	vendor/github.com/containerd/ttrpc/server.go:459 +0x6bd
</span></span><span style=display:flex><span>      1 	vendor/github.com/containerd/ttrpc/server.go:87 +0x107
</span></span><span style=display:flex><span>      2 	vendor/github.com/containerd/ttrpc/server.go:127 +0x2a7
</span></span><span style=display:flex><span>      2 	vendor/github.com/containerd/ttrpc/server.go:332 +0x2ce
</span></span><span style=display:flex><span>      6 	vendor/github.com/containerd/ttrpc/server.go:438 +0xf2
</span></span><span style=display:flex><span>    164 	vendor/github.com/containerd/ttrpc/server.go:444 +0x245
</span></span><span style=display:flex><span>    169 	vendor/github.com/containerd/ttrpc/server.go:434 +0x63f
</span></span></code></pre></div><h2 id=ttrpc>ttrpc <a href=#ttrpc class=anchor>🔗</a></h2><p><strong>reqCh, respCh, msgCh 均为非缓存 channel</strong></p><p>服务端大致如下<p class=markdown-image><img src=https://s2.loli.net/2022/04/21/AwBVbSm9kuL2U1h.png alt></p></p><ul><li>每创建一个连接， 都会产生 2 个协程来处理会话</li><li>recv 协程： 从客户端读取数据，并校验合法性写入 Channel 中，交给 worker 来处理</li><li>worker 协程：收到请求后，<strong>并发创建协程</strong>调用注册的服务</li></ul><p>1.0.1 版本 客户端大致如下<p class=markdown-image><img src=https://s2.loli.net/2022/04/21/WjuCD8eSObodZwY.png alt></p></p><ul><li>和服务端类似，工作协程同时处理接收和发送请求，</li><li>重点是 <strong>发送和接收是在一个协程中处理，并通过内部 waitCall 来同步数据</strong></li></ul><p>发生死锁的情况是 客户端 阻塞在 send 过程，此时因为无法处理返回的 Resp 信息，继而导致服务端的应答数据阻塞在 net write buffer 中
什么情况会导致 send 阻塞，网络发送过程有以下情况 , 进程将数据拷贝到内核缓存区，之后由软中断发送出去，该控制写缓存大小为 tcp_wmem, 内核参数配置为 <code>4096 16384 4194304</code></p><p>针对上述导致死锁的情况，有一个相关 patch[2] 解决，该 patch 修改方式如下图</p><p>1.1.0 版本 客户端<p class=markdown-image><img src=https://s2.loli.net/2022/04/21/Uc5GHopBs1brWDA.png alt></p></p><p>发送和接收 都通过不同的协程处理，不再出现竞争情况发生</p><h1 id=参考>参考 <a href=#%e5%8f%82%e8%80%83 class=anchor>🔗</a></h1><p>[1]. <a href=https://gist.github.com/yylt/0d3f2d554fa7eddd9cafe406ef0c9d75 target=_blank rel=noopener>https://gist.github.com/yylt/0d3f2d554fa7eddd9cafe406ef0c9d75</a>
[2]. <a href=https://github.com/containerd/ttrpc/pull/94 target=_blank rel=noopener>https://github.com/containerd/ttrpc/pull/94</a></p></div><div class=tags><a href=https://easystack.github.io/tags/containerd>containerd</a>
<a href=https://easystack.github.io/tags/kubelete>kubelete</a></div><div id=comment></div></section></main><footer id=footer><div id=social><a class=symbol href=easystack rel=me target=_blank><svg fill="#bbb" width="28" height="28" viewBox="0 0 72 72" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>Github</title><desc>Created with Sketch.</desc><defs/><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)"><g id="Github" transform="translate(264.000000, 939.000000)"><path d="M8 72H64c4.418278.0 8-3.581722 8-8V8c0-4.418278-3.581722-8-8-8H8c-4.418278 811624501e-24-8 3.581722-8 8V64c541083001e-24 4.418278 3.581722 8 8 8z" id="Rounded" fill="#bbb"/><path d="M35.9985 13C22.746 13 12 23.7870921 12 37.096644c0 10.6440272 6.876 19.6751861 16.4145 22.8617681C29.6145 60.1797862 30.0525 59.4358488 30.0525 58.7973276 30.0525 58.2250681 30.0315 56.7100863 30.0195 54.6996482c-6.6765 1.4562499-8.085-3.2302544-8.085-3.2302544-1.0905-2.7829884-2.664-3.5239139-2.664-3.5239139C17.091 46.4500754 19.4355 46.4801943 19.4355 46.4801943c2.4075.1701719 3.675 2.4833051 3.675 2.4833051 2.142 3.6820383 5.6175 2.6188404 6.9855 2.0014024C30.3135 49.4077535 30.9345 48.3460615 31.62 47.7436831 26.2905 47.1352808 20.688 45.0691228 20.688 35.8361671c0-2.6308879.9345-4.781379 2.4705-6.4665327C22.911 28.7597262 22.0875 26.3110578 23.3925 22.9934585c0 0 2.016-.6475568 6.6 2.4697516C31.908 24.9285993 33.96 24.6620468 36.0015 24.6515052 38.04 24.6620468 40.0935 24.9285993 42.0105 25.4632101c4.581-3.1173084 6.5925-2.4697516 6.5925-2.4697516C49.9125 26.3110578 49.089 28.7597262 48.8415 29.3696344 50.3805 31.0547881 51.309 33.2052792 51.309 35.8361671c0 9.2555448-5.6115 11.29309-10.9575 11.8894446.860999999999997.7439374 1.629 2.2137408 1.629 4.4621184C41.9805 55.4089489 41.9505 58.0067059 41.9505 58.7973276 41.9505 59.4418726 42.3825 60.1918338 43.6005 59.9554002 53.13 56.7627944 60 47.7376593 60 37.096644 60 23.7870921 49.254 13 35.9985 13" fill="#fff"/></g></g></g></svg></a></div><div class=copyright>© Copyright
2022
<span class=split><svg fill="#bbb" width="15" height="15" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 15 15"><path d="M13.91 6.75c-1.17 2.25-4.3 5.31-6.07 6.94-.1903.1718-.4797.1718-.67.0C5.39 12.06 2.26 9 1.09 6.75-1.48 1.8 5-1.5 7.5 3.45 10-1.5 16.48 1.8 13.91 6.75z"/></svg></span>easystack</div><div class=powerby>Powered by <a href=http://www.gohugo.io/>Hugo</a> Theme By <a href=https://github.com/nodejh/hugo-theme-cactus-plus>nodejh</a></div></footer></body></html>